{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'name' : 'Alice',\n",
    "} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(data \u001b[39m=\u001b[39;49m data)\n\u001b[0;32m      3\u001b[0m df\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\juanm\\anaconda3\\envs\\py11\\Lib\\site-packages\\pyspark\\sql\\session.py:675\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    672\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(\n\u001b[0;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 675\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mc:\\Users\\juanm\\anaconda3\\envs\\py11\\Lib\\site-packages\\pyspark\\sql\\session.py:700\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    698\u001b[0m     rdd, schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    699\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 700\u001b[0m     rdd, schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[0;32m    701\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n\u001b[0;32m    702\u001b[0m jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[39m.\u001b[39mrdd(), schema\u001b[39m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\juanm\\anaconda3\\envs\\py11\\Lib\\site-packages\\pyspark\\sql\\session.py:512\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    509\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[0;32m    511\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 512\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    513\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    514\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32mc:\\Users\\juanm\\anaconda3\\envs\\py11\\Lib\\site-packages\\pyspark\\sql\\session.py:439\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcan not infer schema from empty dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 439\u001b[0m schema \u001b[39m=\u001b[39m reduce(_merge_type, (_infer_schema(row, names) \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m data))\n\u001b[0;32m    440\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    441\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\juanm\\anaconda3\\envs\\py11\\Lib\\site-packages\\pyspark\\sql\\session.py:439\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcan not infer schema from empty dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 439\u001b[0m schema \u001b[39m=\u001b[39m reduce(_merge_type, (_infer_schema(row, names) \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m data))\n\u001b[0;32m    440\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    441\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\juanm\\anaconda3\\envs\\py11\\Lib\\site-packages\\pyspark\\sql\\types.py:1067\u001b[0m, in \u001b[0;36m_infer_schema\u001b[1;34m(row, names)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     items \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(row\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mitems())\n\u001b[0;32m   1066\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan not infer schema for type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(row))\n\u001b[0;32m   1069\u001b[0m fields \u001b[39m=\u001b[39m []\n\u001b[0;32m   1070\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m items:\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data = data)\n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
